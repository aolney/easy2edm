{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd15aa6",
   "metadata": {},
   "source": [
    "# easy2edm\n",
    "\n",
    "This notebook creates proceedings for the [International Conference on Educational Data Mining](https://educationaldatamining.org/conferences/) using reviewing data from [EasyChair](https://easychair.org).\n",
    "\n",
    "See the [README](README.md) for how to download and structure the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bb5fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy2acl.py - Convert data from EasyChair for use with ACLPUB\n",
    "#\n",
    "# Original Author: Nils Blomqvist\n",
    "# Forked/modified by: Asad Sayeed\n",
    "# Further modifications and docs (for 2019 Anthology): Matt Post\n",
    "# Index for LaTeX book proceedings: Mehdi Ghanimifard and Simon Dobnik\n",
    "# Modified for EDM by Andrew Olney \n",
    "# Please see the documentation in the README file at http://github.com/acl-org/easy2acl.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from csv import DictReader\n",
    "from glob import glob\n",
    "from shutil import copy, rmtree\n",
    "from unicode_tex import unicode_to_tex\n",
    "from pybtex.database import BibliographyData, Entry\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "# Specify conference tracks  here\n",
    "tracks =  ['doctoral-consortium','industry-track','long-papers','posters','short-papers']\n",
    "\n",
    "# specify decision types here\n",
    "decisions = {\n",
    "    \"Accept in current track\" : None,\n",
    "    \"Accept+move to short\" :\"short-papers\",\n",
    "    \"Accept+move to posters\":\"posters\",\n",
    "    \"ACCEPT\":None\n",
    "}\n",
    "\n",
    "def texify(string):\n",
    "    \"\"\"Return a modified version of the argument string where non-ASCII symbols have\n",
    "    been converted into LaTeX escape codes.\n",
    "\n",
    "    \"\"\"\n",
    "    return ' '.join(map(unicode_to_tex, string.split())).replace(r'\\textquotesingle', \"'\")\n",
    "\n",
    "def get_track_metadata(directory):\n",
    "    #,----\n",
    "    #| Metadata\n",
    "    #`----\n",
    "    metadata = { 'chairs': [] }\n",
    "    with open(os.path.join(directory, 'meta')) as metadata_file:\n",
    "        for line in metadata_file:\n",
    "            key, value = line.rstrip().split(maxsplit=1)\n",
    "            if key == 'chairs':\n",
    "                metadata[key].append(value)\n",
    "            else:\n",
    "                metadata[key] = value\n",
    "\n",
    "    for key in 'abbrev volume title shortbooktitle booktitle month year location publisher chairs'.split():\n",
    "        if key not in metadata:\n",
    "            print('Fatal: missing key \"{}\" from \"meta\" file'.format(key))\n",
    "            print(\"Please see the documentation at https://acl-org.github.io/ACLPUB/anthology.html.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    for key in \"bib_url volume_name short_booktitle type\".split():\n",
    "        if key in metadata:\n",
    "            print('Fatal: bad key \"{}\" in the \"meta\" file'.format(key))\n",
    "            print(\"Please see the documentation at https://acl-org.github.io/ACLPUB/anthology.html.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    venue = metadata[\"abbrev\"]\n",
    "    volume_name = metadata[\"volume\"]\n",
    "    year = metadata[\"year\"]\n",
    "    return metadata\n",
    "\n",
    "def collect_track_metadata():\n",
    "    metadata ={}\n",
    "    for d in tracks :\n",
    "        metadata[d] = get_track_metadata(d)\n",
    "    return metadata\n",
    "\n",
    "# Across all tracks, build a dictionary of submissions (which has author \n",
    "# information). We do this across tracks because some submissions have \n",
    "# decisions that move them to other tracks \n",
    "\n",
    "def collect_submissions_and_acceptances( decision_map, metadata ):\n",
    "    submissions = {}\n",
    "    for d in tracks :\n",
    "        with open(os.path.join(d, 'submissions')) as submissions_file:\n",
    "            for line in submissions_file:\n",
    "                entry = line.rstrip().split(\"\\t\")\n",
    "                submission_id = entry[0]\n",
    "                authors = entry[1].replace(' and', ',').split(', ')\n",
    "                title = entry[2]\n",
    "\n",
    "                submissions[submission_id] = (title, authors)\n",
    "            print(\"Found \", len(submissions), \" submitted files in \", d)\n",
    "\n",
    "    #\n",
    "    # Append each accepted submission, as a tuple, to the 'accepted' list.\n",
    "    # Order in this file is used to determine program order.\n",
    "    #\n",
    "    accepted = []\n",
    "    for d in tracks :\n",
    "        with open(os.path.join(d, 'accepted')) as accepted_file:\n",
    "            for line in accepted_file:\n",
    "                entry = line.rstrip().split(\"\\t\")\n",
    "                # modified here to filter out the rejected files rather than doing\n",
    "                # that by hand\n",
    "                #if entry[-1] == 'ACCEPT':\n",
    "                if entry[-1] in decision_map:\n",
    "                    #print(d)\n",
    "                    submission_id = entry[0]\n",
    "                    title = entry[1]\n",
    "                    authors = submissions[submission_id][1]\n",
    "                    # if we defined an explicit mapping, use it\n",
    "                    if decision_map[ entry[-1] ]:\n",
    "                        track = decision_map[ entry[-1] ]\n",
    "                    # otherwise we should place in current track\n",
    "                    else:\n",
    "                        track = d\n",
    "\n",
    "                    accepted.append((submission_id, title, authors, track))\n",
    "            print(\"Found \", len(accepted), \" accepted files in \", d)\n",
    "\n",
    "    # Read abstracts\n",
    "    abstracts = {}\n",
    "    for d in tracks :\n",
    "        if os.path.exists(os.path.join(d, 'submission.csv')):\n",
    "            with open(os.path.join(d, 'submission.csv')) as csv_file:\n",
    "                d = DictReader(csv_file)\n",
    "                for row in d:\n",
    "                    abstracts[row['#']] = row['abstract']\n",
    "            print('Found ', len(abstracts), 'abstracts in ',d)\n",
    "        else:\n",
    "            print('No abstracts available.')\n",
    "\n",
    "    #\n",
    "    # Find all relevant PDFs\n",
    "    #\n",
    "    venue = metadata['long-papers'][\"abbrev\"]\n",
    "    year = metadata['long-papers'][\"year\"]\n",
    "    booktitle = metadata['long-papers']['booktitle']\n",
    "    chairs = metadata['long-papers']['chairs']\n",
    "    \n",
    "    # The PDF of the full proceedings\n",
    "    full_pdf_file = 'pdf/{}_{}.pdf'.format(venue, year)\n",
    "    if not os.path.exists(full_pdf_file):\n",
    "        print(\"Fatal: could not find full volume PDF '{}'\".format(full_pdf_file))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # The PDF of the frontmatter\n",
    "    frontmatter_pdf_file = 'pdf/{}_{}_frontmatter.pdf'.format(venue, year)\n",
    "    if not os.path.exists(frontmatter_pdf_file):\n",
    "        print(\"Fatal: could not find frontmatter PDF file '{}'\".format(frontmatter_pdf_file))\n",
    "        sys.exit(1)\n",
    "\n",
    "    # File locations of all PDFs (seeded with PDF for frontmatter)\n",
    "    pdfs = { '0': frontmatter_pdf_file }\n",
    "    for d in tracks :\n",
    "        for pdf_file in glob(os.path.join(d,'pdf/{}_{}_paper_*.pdf'.format(venue, year))):\n",
    "            submission_id = pdf_file.split('_')[-1].replace('.pdf', '')\n",
    "            pdfs[submission_id] = pdf_file\n",
    "\n",
    "    # List of accepted papers (seeded with frontmatter)\n",
    "    accepted.insert(0, ('0', booktitle, chairs))\n",
    "    return (submissions, accepted, abstracts, pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc2bf8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  13  submitted files in  doctoral-consortium\n",
      "Found  19  submitted files in  industry-track\n",
      "Found  109  submitted files in  long-papers\n",
      "Found  129  submitted files in  posters\n",
      "Found  185  submitted files in  short-papers\n",
      "Found  9  accepted files in  doctoral-consortium\n",
      "Found  13  accepted files in  industry-track\n",
      "Found  65  accepted files in  long-papers\n",
      "Found  75  accepted files in  posters\n",
      "Found  112  accepted files in  short-papers\n",
      "Found  13 abstracts in  <csv.DictReader object at 0x7f016611f790>\n",
      "Found  19 abstracts in  <csv.DictReader object at 0x7f016611f750>\n",
      "Found  109 abstracts in  <csv.DictReader object at 0x7f016611fed0>\n",
      "Found  129 abstracts in  <csv.DictReader object at 0x7f016611f890>\n",
      "Found  185 abstracts in  <csv.DictReader object at 0x7f016611fed0>\n"
     ]
    }
   ],
   "source": [
    "metadata =collect_track_metadata()\n",
    "\n",
    "submissions, accepted, abstracts, pdfs = collect_submissions_and_acceptances( decisions, metadata )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607aee7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xpython",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
